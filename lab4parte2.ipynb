{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850356b3",
   "metadata": {},
   "source": [
    "# Laboratorio 4\n",
    "\n",
    "Sean bienvenidos de nuevo al laboratorio 4 de Deep Learning y Sistemas Inteligentes. Así como en los laboratorios pasados, espero que esta ejercitación les sirva para consolidar sus conocimientos en el tema de Encoder-Decoder y AutoEnconders.\n",
    "\n",
    "Para este laboratorio estaremos usando una herramienta para Jupyter Notebooks que facilitará la calificación, no solo asegurándo que ustedes tengan una nota pronto sino también mostrandoles su nota final al terminar el laboratorio.\n",
    "\n",
    "Espero que esta vez si se muestren los *marks*. De nuevo me discupo si algo no sale bien, seguiremos mejorando conforme vayamos iterando. Siempre pido su comprensión y colaboración si algo no funciona como debería. \n",
    "\n",
    "Al igual que en el laboratorio pasado, estaremos usando la librería de Dr John Williamson et al de la University of Glasgow, además de ciertas piezas de código de Dr Bjorn Jensen de su curso de Introduction to Data Science and System de la University of Glasgow para la visualización de sus calificaciones. \n",
    "\n",
    "**NOTA:** Ahora tambien hay una tercera dependecia que se necesita instalar. Ver la celda de abajo por favor\n",
    "\n",
    "<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49183e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:54.419993Z",
     "start_time": "2023-08-06T06:29:54.409473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/johnhw/jhwutils/zipball/master\n",
      "  Downloading https://github.com/johnhw/jhwutils/zipball/master\n",
      "     - 0 bytes ? 0:00:00\n",
      "     - 11.6 kB ? 0:00:00\n",
      "     - 11.6 kB ? 0:00:00\n",
      "     - 38.1 kB 444.6 kB/s 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: jhwutils\n",
      "  Building wheel for jhwutils (pyproject.toml): started\n",
      "  Building wheel for jhwutils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jhwutils: filename=jhwutils-1.0-py3-none-any.whl size=33803 sha256=d6a6020667e06f265d82363eedb647d8ff6b0c50564b4dafa03513271b8b6345\n",
      "  Stored in directory: C:\\Users\\andre\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-def85os1\\wheels\\22\\cf\\fc\\464198e5e7ba125a8fc9bb20e6297eb4deac9061eda6860554\n",
      "Successfully built jhwutils\n",
      "Installing collected packages: jhwutils\n",
      "Successfully installed jhwutils-1.0\n",
      "Collecting scikit-image\n",
      "  Obtaining dependency information for scikit-image from https://files.pythonhosted.org/packages/32/b2/1811645651153407f1e715b75afe9962d87582bee70b42c8671c255f8fe6/scikit_image-0.21.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scikit_image-0.21.0-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from scikit-image) (1.25.2)\n",
      "Collecting scipy>=1.8 (from scikit-image)\n",
      "  Obtaining dependency information for scipy>=1.8 from https://files.pythonhosted.org/packages/96/9b/10048be0c335327077af430c5a6637c0b9e7fe9121a8048836f1bb022a81/scipy-1.11.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.1-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.1 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.1 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/59.1 kB 262.6 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 41.0/59.1 kB 196.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.1/59.1 kB 261.5 kB/s eta 0:00:00\n",
      "Collecting networkx>=2.8 (from scikit-image)\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.1/2.1 MB 880.9 kB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.1/2.1 MB 880.9 kB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.1/2.1 MB 554.9 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.2/2.1 MB 737.3 kB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.2/2.1 MB 737.3 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.2/2.1 MB 655.6 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.2/2.1 MB 655.6 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.2/2.1 MB 533.8 kB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 0.3/2.1 MB 587.7 kB/s eta 0:00:04\n",
      "     -------- ------------------------------- 0.4/2.1 MB 771.4 kB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 0.5/2.1 MB 885.8 kB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.5/2.1 MB 880.7 kB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.6/2.1 MB 899.0 kB/s eta 0:00:02\n",
      "     ------------ --------------------------- 0.7/2.1 MB 932.2 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.7/2.1 MB 942.1 kB/s eta 0:00:02\n",
      "     --------------- ------------------------ 0.8/2.1 MB 1.0 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 0.9/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 1.0/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.1/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.1/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.2/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.3/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.4/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.5/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.5/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.6/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.7/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.8/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.9/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 2.0/2.1 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting pillow>=9.0.1 (from scikit-image)\n",
      "  Obtaining dependency information for pillow>=9.0.1 from https://files.pythonhosted.org/packages/8f/b8/1bf1012eee3059d150194d1fab148f553f3df42cf412e4e6656c772afad9/Pillow-10.0.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp39-cp39-win_amd64.whl.metadata (9.6 kB)\n",
      "Collecting imageio>=2.27 (from scikit-image)\n",
      "  Obtaining dependency information for imageio>=2.27 from https://files.pythonhosted.org/packages/c7/b0/7b6c35b8636ed773325cdb6f5ac3cd36afba63d99e20ed59c521cf5018b4/imageio-2.31.1-py3-none-any.whl.metadata\n",
      "  Downloading imageio-2.31.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Obtaining dependency information for tifffile>=2022.8.12 from https://files.pythonhosted.org/packages/74/68/19989a1009f68ed777ea5d2624c2996bab0890a31ce7d4b2a7ae4e1c0cfe/tifffile-2023.8.12-py3-none-any.whl.metadata\n",
      "  Downloading tifffile-2023.8.12-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image)\n",
      "  Using cached PyWavelets-1.4.1-cp39-cp39-win_amd64.whl (4.2 MB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from scikit-image) (23.1)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image)\n",
      "  Obtaining dependency information for lazy_loader>=0.2 from https://files.pythonhosted.org/packages/a1/c3/65b3814e155836acacf720e5be3b5757130346670ac454fee29d3eda1381/lazy_loader-0.3-py3-none-any.whl.metadata\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached scikit_image-0.21.0-cp39-cp39-win_amd64.whl (22.9 MB)\n",
      "Using cached imageio-2.31.1-py3-none-any.whl (313 kB)\n",
      "Using cached lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Downloading Pillow-10.0.0-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/2.5 MB 2.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.6/2.5 MB 2.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.6/2.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.8/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.1/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.5/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.6/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.9/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.0/2.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.3/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.1-cp39-cp39-win_amd64.whl (44.1 MB)\n",
      "   ---------------------------------------- 0.0/44.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/44.1 MB 2.2 MB/s eta 0:00:21\n",
      "   ---------------------------------------- 0.2/44.1 MB 3.0 MB/s eta 0:00:15\n",
      "   ---------------------------------------- 0.5/44.1 MB 3.9 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/44.1 MB 3.5 MB/s eta 0:00:13\n",
      "    --------------------------------------- 0.7/44.1 MB 3.0 MB/s eta 0:00:15\n",
      "    --------------------------------------- 0.7/44.1 MB 2.8 MB/s eta 0:00:16\n",
      "    --------------------------------------- 0.8/44.1 MB 2.5 MB/s eta 0:00:18\n",
      "    --------------------------------------- 0.9/44.1 MB 2.4 MB/s eta 0:00:19\n",
      "    --------------------------------------- 1.0/44.1 MB 2.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 1.1/44.1 MB 2.5 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.2/44.1 MB 2.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 1.2/44.1 MB 2.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.3/44.1 MB 2.1 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 1.4/44.1 MB 2.1 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 1.5/44.1 MB 2.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.6/44.1 MB 2.3 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 1.8/44.1 MB 2.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.8/44.1 MB 2.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.8/44.1 MB 2.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.9/44.1 MB 2.0 MB/s eta 0:00:22\n",
      "   - -------------------------------------- 2.0/44.1 MB 2.0 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 2.1/44.1 MB 2.1 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 2.3/44.1 MB 2.1 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 2.3/44.1 MB 2.1 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 2.4/44.1 MB 2.1 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 2.6/44.1 MB 2.2 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 2.8/44.1 MB 2.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 3.0/44.1 MB 2.3 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 3.1/44.1 MB 2.3 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 3.2/44.1 MB 2.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.3/44.1 MB 2.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.5/44.1 MB 2.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.6/44.1 MB 2.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.7/44.1 MB 2.3 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 3.9/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 4.0/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 4.2/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 4.4/44.1 MB 2.5 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 4.4/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.6/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.7/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.7/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.9/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 5.1/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 5.1/44.1 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 5.1/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 5.3/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 5.4/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 5.4/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.5/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.6/44.1 MB 2.4 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.6/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.7/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.7/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 5.8/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.0/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.2/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.2/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.3/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.3/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.3/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.3/44.1 MB 2.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.3/44.1 MB 2.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 6.5/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 6.5/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 6.7/44.1 MB 2.2 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 6.7/44.1 MB 2.2 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 6.7/44.1 MB 2.2 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 7.0/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 7.0/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 7.0/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 7.2/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 7.3/44.1 MB 2.2 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 7.5/44.1 MB 2.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 7.8/44.1 MB 2.3 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 8.0/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.0/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.1/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.2/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.4/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.6/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.7/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 8.7/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 8.8/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 9.0/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 9.2/44.1 MB 2.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 9.5/44.1 MB 2.3 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 9.8/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 9.8/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 9.8/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.0/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.1/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.1/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.2/44.1 MB 2.3 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.3/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.7/44.1 MB 2.4 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 10.9/44.1 MB 2.4 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 11.1/44.1 MB 2.4 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 11.2/44.1 MB 2.4 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 11.3/44.1 MB 2.4 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 11.4/44.1 MB 2.5 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 11.7/44.1 MB 2.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 11.9/44.1 MB 2.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 12.1/44.1 MB 2.6 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 12.4/44.1 MB 2.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 12.8/44.1 MB 2.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 12.8/44.1 MB 2.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 13.0/44.1 MB 2.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 13.2/44.1 MB 2.7 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 13.3/44.1 MB 2.8 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 13.5/44.1 MB 2.8 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 13.8/44.1 MB 2.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 14.0/44.1 MB 2.8 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 14.2/44.1 MB 2.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 14.4/44.1 MB 2.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 14.6/44.1 MB 2.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 14.8/44.1 MB 2.9 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 15.1/44.1 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 15.1/44.1 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 15.3/44.1 MB 2.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 15.5/44.1 MB 3.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 15.8/44.1 MB 3.1 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 16.0/44.1 MB 3.2 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 16.0/44.1 MB 3.2 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 16.4/44.1 MB 3.3 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.7/44.1 MB 3.6 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 16.9/44.1 MB 3.6 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 17.2/44.1 MB 3.8 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 17.5/44.1 MB 3.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 17.7/44.1 MB 3.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 17.9/44.1 MB 3.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 18.1/44.1 MB 3.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 18.3/44.1 MB 4.0 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 18.5/44.1 MB 4.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 18.8/44.1 MB 4.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 18.9/44.1 MB 4.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 19.2/44.1 MB 4.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 19.3/44.1 MB 4.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 19.5/44.1 MB 4.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 20.0/44.1 MB 4.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 20.0/44.1 MB 4.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 20.0/44.1 MB 4.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 20.3/44.1 MB 4.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 20.5/44.1 MB 4.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 21.0/44.1 MB 4.5 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 21.4/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 21.5/44.1 MB 4.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 21.8/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 22.1/44.1 MB 4.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 22.3/44.1 MB 4.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 22.4/44.1 MB 4.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 22.7/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 22.9/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 23.3/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 23.7/44.1 MB 5.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 23.8/44.1 MB 5.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 23.9/44.1 MB 4.9 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 24.0/44.1 MB 4.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 24.0/44.1 MB 4.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 24.1/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 24.4/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 24.4/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 24.6/44.1 MB 4.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 25.0/44.1 MB 4.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 25.1/44.1 MB 4.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 25.4/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 25.6/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 25.9/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 26.1/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 26.3/44.1 MB 4.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 26.4/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 26.8/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 27.2/44.1 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 27.2/44.1 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 27.2/44.1 MB 4.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 27.7/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 27.7/44.1 MB 4.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 28.0/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 28.2/44.1 MB 4.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 28.5/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 28.9/44.1 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 29.3/44.1 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 29.3/44.1 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 29.8/44.1 MB 4.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.0/44.1 MB 4.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.2/44.1 MB 4.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.5/44.1 MB 5.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.9/44.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.0/44.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.5/44.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.9/44.1 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.9/44.1 MB 5.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 32.1/44.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 32.5/44.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 32.8/44.1 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 33.2/44.1 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 33.5/44.1 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 33.8/44.1 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 34.0/44.1 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.4/44.1 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.6/44.1 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.9/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 35.0/44.1 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.5/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.6/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.9/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 36.0/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 36.2/44.1 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.5/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.8/44.1 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.9/44.1 MB 5.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 37.3/44.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 37.5/44.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 37.7/44.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 38.1/44.1 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 38.3/44.1 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.8/44.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.0/44.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.0/44.1 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.5/44.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.5/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.9/44.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.1/44.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.2/44.1 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.4/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.6/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.0/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.0/44.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.3/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.3/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.3/44.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.5/44.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.7/44.1 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.0/44.1 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.2/44.1 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.6/44.1 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.9/44.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.0/44.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.4/44.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/44.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.1/44.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.1/44.1 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading tifffile-2023.8.12-py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/221.0 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 143.4/221.0 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 221.0/221.0 kB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: tifffile, scipy, PyWavelets, pillow, networkx, lazy_loader, imageio, scikit-image\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.31.1 lazy_loader-0.3 networkx-3.1 pillow-10.0.0 scikit-image-0.21.0 scipy-1.11.1 tifffile-2023.8.12\n",
      "Collecting https://github.com/AlbertS789/lautils/zipball/master\n",
      "  Downloading https://github.com/AlbertS789/lautils/zipball/master\n",
      "     - 0 bytes ? 0:00:00\n",
      "     - 4.2 kB ? 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: lautils\n",
      "  Building wheel for lautils (pyproject.toml): started\n",
      "  Building wheel for lautils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for lautils: filename=lautils-1.0-py3-none-any.whl size=2833 sha256=f7caadd94117146e990780710a831ec3dfe9d692811331fb18127bd777408520\n",
      "  Stored in directory: C:\\Users\\andre\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-raz96mc7\\wheels\\2a\\48\\f3\\ce4089427ce7f6d240b19ac257e642e536e93232f7e6b7e1b5\n",
      "Successfully built lautils\n",
      "Installing collected packages: lautils\n",
      "Successfully installed lautils-1.0\n"
     ]
    }
   ],
   "source": [
    "# Una vez instalada la librería por favor, recuerden volverla a comentar.\n",
    "!pip install -U --force-reinstall --no-cache https://github.com/johnhw/jhwutils/zipball/master\n",
    "!pip install scikit-image\n",
    "!pip install -U --force-reinstall --no-cache https://github.com/AlbertS789/lautils/zipball/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aca9e199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/c9/46/6cbaf20f5bd0e7c1d204b45b853c2cd317b303fada90245f2825ecca47de/matplotlib-3.7.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading matplotlib-3.7.2-cp39-cp39-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/a5/d6/80258c2759bd34abe267b5d3bc6300f7105aa70181b99d531283f7e7c79e/contourpy-1.1.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading contourpy-1.1.0-cp39-cp39-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/cd/b1/8ba85c3d50562438d5991f5698d46b66dcadd43d230c7ba72edbd0c96ce8/fonttools-4.42.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading fonttools-4.42.0-cp39-cp39-win_amd64.whl.metadata (153 kB)\n",
      "     ---------------------------------------- 0.0/153.7 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/153.7 kB ? eta -:--:--\n",
      "     ------- ----------------------------- 30.7/153.7 kB 435.7 kB/s eta 0:00:01\n",
      "     ------- ----------------------------- 30.7/153.7 kB 435.7 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 41.0/153.7 kB 245.8 kB/s eta 0:00:01\n",
      "     ---------------------- -------------- 92.2/153.7 kB 403.5 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 143.4/153.7 kB 532.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 153.7/153.7 kB 510.4 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.4-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from matplotlib) (10.0.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for importlib-resources>=3.2.0 from https://files.pythonhosted.org/packages/25/d4/592f53ce2f8dde8be5720851bd0ab71cc2e76c55978e4163ef1ab7e389bb/importlib_resources-6.0.1-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.7.2-cp39-cp39-win_amd64.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/7.5 MB 1.3 MB/s eta 0:00:06\n",
      "   ---------------------------------------- 0.1/7.5 MB 812.7 kB/s eta 0:00:10\n",
      "    --------------------------------------- 0.1/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.1/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.2/7.5 MB 980.4 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/7.5 MB 983.0 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/7.5 MB 980.4 kB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/7.5 MB 952.6 kB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.4/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.4/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.6/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.6/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.7/7.5 MB 1.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.8/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.8/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.8/7.5 MB 1.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.0/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.0/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.1/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.1/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.2/7.5 MB 1.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.3/7.5 MB 1.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.4/7.5 MB 1.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.4/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.5/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.6/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.7/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.7/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.8/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.9/7.5 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.0/7.5 MB 1.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.0/7.5 MB 1.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.1/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.2/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.3/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.4/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.4/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 2.4/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.7/7.5 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.7/7.5 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 2.7/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.8/7.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.9/7.5 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.0/7.5 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.0/7.5 MB 1.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.2/7.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.3/7.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.5/7.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.6/7.5 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.6/7.5 MB 1.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.8/7.5 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 3.9/7.5 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.0/7.5 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.1/7.5 MB 1.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 4.3/7.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.4/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.5/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.7/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.8/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.9/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.0/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.2/7.5 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.2/7.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.3/7.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.4/7.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.6/7.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.7/7.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.8/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.8/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.9/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.9/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.9/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.2/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.3/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.4/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.6/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.6/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.7/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.9/7.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.9/7.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.0/7.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.3/7.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.4/7.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.5/7.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.1.0-cp39-cp39-win_amd64.whl (429 kB)\n",
      "   ---------------------------------------- 0.0/429.4 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 153.6/429.4 kB 4.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 337.9/429.4 kB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 389.1/429.4 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 429.4/429.4 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading fonttools-4.42.0-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.1 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.0 importlib-resources-6.0.1 kiwisolver-1.4.4 matplotlib-3.7.2 pyparsing-3.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c2378f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T22:24:37.953793Z",
     "start_time": "2023-08-07T22:24:34.644956Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "#from IPython import display\n",
    "#from base64 import b64decode\n",
    "\n",
    "\n",
    "# Other imports\n",
    "from unittest.mock import patch\n",
    "from uuid import getnode as get_mac\n",
    "\n",
    "from jhwutils.checkarr import array_hash, check_hash, check_scalar, check_string, array_hash, _check_scalar\n",
    "import jhwutils.image_audio as ia\n",
    "import jhwutils.tick as tick\n",
    "from lautils.gradeutils import new_representation, hex_to_float, compare_numbers, compare_lists_by_percentage, calculate_coincidences_percentage\n",
    "\n",
    "###\n",
    "tick.reset_marks()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872e6c48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:55.567829Z",
     "start_time": "2023-08-06T06:29:55.560965Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29e52b805cfebe42903d0379a3f485da",
     "grade": false,
     "grade_id": "cell-95b81aaa3e57306b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Seeds\n",
    "seed_ = 2023\n",
    "np.random.seed(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e571e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:55.581630Z",
     "start_time": "2023-08-06T06:29:55.567829Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3aa8961ba46ffd91e0ae666686e967e7",
     "grade": true,
     "grade_id": "cell-b2ae10e4b3198bb2",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Celda escondida para utlidades necesarias, por favor NO edite esta celda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97c050",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea27899c011d00466ba84d10df3c8450",
     "grade": false,
     "grade_id": "cell-37707c73cc6055e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###### Información del estudiante en dos variables\n",
    "\n",
    "* carne_1 : un string con su carne (e.g. \"12281\"), debe ser de al menos 5 caracteres.\n",
    "* firma_mecanografiada_1: un string con su nombre (e.g. \"Albero Suriano\") que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)\n",
    "* carne_2 : un string con su carne (e.g. \"12281\"), debe ser de al menos 5 caracteres.\n",
    "* firma_mecanografiada_2: un string con su nombre (e.g. \"Albero Suriano\") que se usará para la declaracion que este trabajo es propio (es decir, no hay plagio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e766e448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:55.588643Z",
     "start_time": "2023-08-06T06:29:55.581630Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7c7bd38d70a53f41a59434e097ebf75",
     "grade": false,
     "grade_id": "cell-887917342d3eaa54",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# carne_1 = \n",
    "# firma_mecanografiada_1 = \n",
    "# carne_2 = \n",
    "# firma_mecanografiada_2 = \n",
    "# YOUR CODE HERE\n",
    "carne_1 = \"20074\"\n",
    "firma_mecanografiada_1 = \"Mariano Reyes\"\n",
    "carne_2 = \"20102\"\n",
    "firma_mecanografiada_2 = \"Andrea Lam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d41a5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:55.602639Z",
     "start_time": "2023-08-06T06:29:55.588643Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6069d482a40ebc901473d44861baeb63",
     "grade": true,
     "grade_id": "cell-4aa33cdbf61b184d",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"0\"}--> \n",
       "         ✓ [0 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"0\"}--> \n",
       "         ✓ [0 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deberia poder ver dos checkmarks verdes [0 marks], que indican que su información básica está OK \n",
    "\n",
    "with tick.marks(0): \n",
    "    assert(len(carne_1)>=5 and len(carne_2)>=5)\n",
    "\n",
    "with tick.marks(0):  \n",
    "    assert(len(firma_mecanografiada_1)>0 and len(firma_mecanografiada_2)>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c98973",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c296a7f61dad354a3388f85563084bc",
     "grade": false,
     "grade_id": "cell-d37c69d4d3712b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parte 1 - Word2Vec\n",
    "\n",
    "**Créditos:** La primera parte de este laboratorio está tomado y basado en uno de los post de Musashi (Jacobs-) Harukawa\n",
    "\n",
    "La eficacia de las técnicas de embedding está directamente relacionada con los desafíos iniciales que motivaron los enfoques de texto como datos. Al convertir el lenguaje natural en representaciones numéricas, los métodos de incrustación abren oportunidades para aplicar varias herramientas cuantitativas a fuentes de datos previamente sin explotar.\n",
    "\n",
    "En términos generales, word embedding representa cada palabra en un conjunto dado de textos (corpus) como vectores en un espacio k-dimensional (donde k es elegido por el investigador; más detalles sobre esto más adelante). Estos vectores contienen información valiosa sobre las relaciones de las palabras y su contexto, sirviendo como herramientas esenciales para las tareas posteriores de modelado del lenguaje.\n",
    "\n",
    "Entonces, es entendible que se pregunten\n",
    "\n",
    "* ¿Cómo funciona este proceso de incrustación?\n",
    "* ¿Cuál es la razón subyacente de su éxito?\n",
    "* ¿Cómo podemos determinar su eficacia?\n",
    "\n",
    "Para poder responder las primeras dos preguntas, vamos a implementar este modelo usando PyTorch. Noten que el state-of-the-art ya no solo se usa Word2Vec, como BERT (Bidirectional Encoder Representations from Transformers). Pero siempre es un buen ejercicio entender estos algoritmos.\n",
    "\n",
    "### Paso 1 - DataLoader\n",
    "\n",
    "Como en laboratorios, lo primero que necesitamos es definir un DataLoader. Para esta primera parte estaremos usando el dataset llamado \"tweets_hate_speech_detection\" de HugginFace.\n",
    "\n",
    "Para esto necesitamos una función que separe los textos en listas de tokens. El preprocesamiento para cuando se trabaja con textos debe ser un poco más exhaustivo de lo que haremos en este laboratorio, pero para fines del mismo solamente haremos:\n",
    "\n",
    "1- Pasar a minusculas\n",
    "\n",
    "2- Quitar todos los simbolos diferentes de a-z@#\n",
    "\n",
    "3- Separar en espacios\n",
    "\n",
    "4- Quitar \"stopword\" y tokens vacíos\n",
    "\n",
    "5- Aplicar snowball stemmer al resto (snowball? sí, refieran a la nota de abajo para la explicación rápida)\n",
    "\n",
    "Para esto nos apoyaremos en el paquete de natural language processing toolkit o nltk para los cuates. Entonces, recuerden instalarlo por favor \"pip install nltk\"\n",
    "\n",
    "**Snowball Stemmer** es un modulo en la librería NLTK que implementa la técnica de stemming. ¿Stemming?\n",
    "Stemming es una técnica utilizada para extraer la forma base de las palabras mediante la eliminación de los (pre-post)fijos de ellos. Imaginen que cortan la ramas de un árbol hasta los tallos. Por ejemplo, la raíz de las palabras comiendo, come, comido es comer. Refieran a este [link](https://www.tutorialspoint.com/natural_language_toolkit/natural_language_toolkit_stemming_lemmatization.htm) para más información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3520d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b5c601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:58.840015Z",
     "start_time": "2023-08-06T06:29:55.602639Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset('tweets_hate_speech_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f566f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b66ea76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:59.004447Z",
     "start_time": "2023-08-06T06:29:58.840015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Para simplicidad quitemos characteres pero mantegamos @ y #\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ss = SnowballStemmer('english')\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#def split_tokens(row):                             # PASO\n",
    "#    row['all_tokens'] = [ss.stem(i) for i in       # 5\n",
    "#                     re.split(r\" +\",               # 3\n",
    "#                     re.sub(r\"[^a-z@# ]\", \"\",      # 2\n",
    "#                            row['tweet'].lower())) # 1\n",
    "#                     if (i not in sw) and len(i)]  # 4\n",
    "#    return row\n",
    "\n",
    "def split_tokens(row):\n",
    "    # 1- Pasar a minusculas\n",
    "    tweet_lower = row['tweet'].lower()\n",
    "    \n",
    "    # 2- Quitar todos los simbolos diferentes de a-z@#\n",
    "    tweet_cleaned = re.sub(r\"[^a-z@# ]\", \"\", tweet_lower)\n",
    "    \n",
    "    # 3- Separar en espacios\n",
    "    tweet_tokens = re.split(r\" +\", tweet_cleaned)\n",
    "    \n",
    "    # 4- Quitar \"stopword\" y tokens vacíos\n",
    "    # 5- Aplicar snowball stemmer al resto \n",
    "    filtered_tokens = [ss.stem(i) for i in tweet_tokens if (i not in sw) and len(i)]\n",
    "    \n",
    "    \n",
    "    row['all_tokens'] = filtered_tokens\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "186eecad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:59.025478Z",
     "start_time": "2023-08-06T06:29:59.004447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determinamos el vocabulario\n",
    "dataset = dataset.map(split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a92c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T01:41:00.383114Z",
     "start_time": "2023-08-06T01:41:00.367813Z"
    }
   },
   "source": [
    "Ahora podemos crear algunas variables que nos serán útiles en futuros pasos. Además, debemos quitar los tokens que ocurren menos de 10 veces para reducir el tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0837b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:59.486408Z",
     "start_time": "2023-08-06T06:29:59.025478Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d1e8d5c6985acc13aa5f87a951182e5",
     "grade": false,
     "grade_id": "cell-0305720eb97e48ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Total de palabras\n",
    "\n",
    "counts = Counter([i for s in dataset['train']['all_tokens'] for i in s])\n",
    "counts = {k:v for k, v in counts.items() if v>10} # Filtering\n",
    "\n",
    "# Aprox 1 linea para obtener los tokens unicos\n",
    "# vocab = \n",
    "# Hint: Use list de python\n",
    "# Hint2: Use la variable counts\n",
    "# YOUR CODE HERE\n",
    "vocab = list(counts.keys())\n",
    "\n",
    "# Aprox 1 linea para determinar el tamaño del vocabulario\n",
    "# YOUR CODE HERE\n",
    "n_v = len(vocab)\n",
    "\n",
    "# Aprox 2 lineas para definir \n",
    "#     los diccionarios para ir de un token a un id numérico y viceversa\n",
    "# id2tok = \n",
    "# tok2id = \n",
    "# Hint: Puede que dict y enumerate le sirva para una definición\n",
    "# YOUR CODE HERE\n",
    "id2tok = dict(enumerate(vocab))\n",
    "tok2id = {v: k for k, v in id2tok.items()}\n",
    "\n",
    "# Funcion para quitar tokens \"raros\"\n",
    "def remove_rare_tokens(row):\n",
    "    row['tokens'] = [t for t in row['all_tokens'] if t in vocab]\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(remove_rare_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d8a6c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:59.501418Z",
     "start_time": "2023-08-06T06:29:59.487438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'tweet', 'all_tokens', 'tokens'],\n",
       "        num_rows: 31962\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'tweet', 'all_tokens', 'tokens'],\n",
       "        num_rows: 17197\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c79775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:59.517184Z",
     "start_time": "2023-08-06T06:29:59.502434Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4cf221bfa47a4f32352470e32c09b04",
     "grade": true,
     "grade_id": "cell-801ad99c67585892",
     "locked": true,
     "points": 18,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(3):        \n",
    "    assert(check_scalar(len(counts), '0xf4f4eb83'))\n",
    "    \n",
    "with tick.marks(3):        \n",
    "    assert(check_scalar(len(id2tok), '0xf4f4eb83'))\n",
    "    \n",
    "with tick.marks(3):        \n",
    "    assert(check_scalar(len(vocab), '0xf4f4eb83'))\n",
    "    \n",
    "with tick.marks(3):        \n",
    "    assert(check_scalar(n_v, '0xf4f4eb83'))\n",
    "    \n",
    "with tick.marks(3):        \n",
    "    assert(check_scalar(tok2id['father'], '0xb44c37ea'))\n",
    "    \n",
    "with tick.marks(3):        \n",
    "    assert(check_string(id2tok[1], '0xcf2531b8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f310c6",
   "metadata": {},
   "source": [
    "Ahora, recordemos que Word2Vec ayuda a representar una palabra por su contexto, para ello necesitamos definir una ventana movil (sliding window) que se usa dentro del algoritmo. Esta consiste en tomar cada palabra de una frase, y luego se parea con las N palabras más cercanas (hacia la derecha e izquierda). Por ejemplo, consideremos una frase como \"every good dog does fine\", con una ventana de 2. El resultado sería algo como:\n",
    "\n",
    "`(every, good)`\n",
    "`(every, dog)`\n",
    "`(good, every)`\n",
    "`(good, dog)`\n",
    "`(good, does)`\n",
    "`(dog, every)`\n",
    "`(dog, good)`\n",
    "`...`\n",
    "\n",
    "Y así consecutivamente. La frase u oración, es convertida en un par `target, context` donde el contex es una lista de tokens dentro de la ventana.\n",
    "\n",
    "Luego, definiremos el DataSet usando las clases correspondiente como lo hemos hecho antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc3dfec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:29:59.580001Z",
     "start_time": "2023-08-06T06:29:59.520187Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3be587ebad6002016b9a1210d7dfb2d",
     "grade": false,
     "grade_id": "cell-7d04d85a04e5fc7a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31962/31962 [00:05<00:00, 5734.11 examples/s]\n",
      "Map: 100%|██████████| 17197/17197 [00:03<00:00, 5524.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#def windowizer(row, wsize=3):\n",
    "#    \"\"\"\n",
    "#    Windowizer function for Word2Vec. Converts sentence to sliding-window\n",
    "#    pairs.\n",
    "#    \"\"\"\n",
    "#    doc = row['tokens']\n",
    "#    #wsize = 3\n",
    "#    out = []\n",
    "#    for i, word in enumerate(doc):\n",
    "#        target = tok2id[word]\n",
    "#        window = [i+j for j in\n",
    "#                  range(-wsize, wsize+1, 1)\n",
    "#                  if (i+j>=0) &\n",
    "#                     (i+j<len(doc)) &\n",
    "#                     (j!=0)]\n",
    "#\n",
    "#        out += [(target, tok2id[doc[w]]) for w in window]\n",
    "#    row['moving_window'] = out\n",
    "#    return row\n",
    "\n",
    "\n",
    "def windowizer(row, wsize=3):\n",
    "    \"\"\"\n",
    "    Windowizer function for Word2Vec. Converts sentence to sliding-window\n",
    "    pairs.\n",
    "    \"\"\"\n",
    "    doc = row['tokens']\n",
    "    out = []\n",
    "    \n",
    "    for i, word in enumerate(doc):\n",
    "        target = tok2id[word]\n",
    "        \n",
    "        # 1 - Definimos el rango de la ventana movil\n",
    "        window = [i + j for j in range(-wsize, wsize + 1, 1) if (i + j >= 0) & (i + j < len(doc)) & (j != 0)]\n",
    "        \n",
    "        # 2 - Creamos pares de la ventana movil\n",
    "        # Aprox 1 linea\n",
    "        # window_pairs = \n",
    "        # YOUR CODE HERE\n",
    "        window_pairs = [(target, tok2id[doc[w]]) for w in window]\n",
    "        \n",
    "        # 3 - Agregamos los pares a la lista de salida\n",
    "        # Aprox 1 linea \n",
    "        # out +=\n",
    "        # YOUR CODE HERE\n",
    "        out += window_pairs\n",
    "        \n",
    "    # 4 - Asingamos el \"movin_window\" a la fila\n",
    "    row['moving_window'] = out\n",
    "    \n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(windowizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc357ed4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:02.539396Z",
     "start_time": "2023-08-06T06:30:02.518455Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "726e16c824a2e5b29968e3f1bbab59bf",
     "grade": true,
     "grade_id": "cell-66be60a252d35f9d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(5):        \n",
    "    assert(check_scalar(dataset[\"train\"].num_rows, '0xcd61d16b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de5447bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:02.555506Z",
     "start_time": "2023-08-06T06:30:02.539396Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c728e41718e96a23aa6a0f9ce216263",
     "grade": false,
     "grade_id": "cell-1d4097cccc4ceee4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    " \n",
    "    \n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab_size, wsize=3):\n",
    "        self.dataset = dataset\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = [i for s in dataset['moving_window'] for i in s]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx][0], self.data[idx][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e73cd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fa751aadaad5be9ec05ab3c4f6e31a4",
     "grade": false,
     "grade_id": "cell-249f95b6dcc39bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora definiremos dos variables globales, el `BATCH_SIZE` y `N_LOADER_PROCS`.\n",
    "\n",
    "`BATCH_SIZE` es el número de observaciones devueltas con cada llamada. Gran parte de las aceleraciones del procesamiento de GPU provienen de cálculos de matriz por batches masivos. Al elegir el tamaño del batch, recuerden que generalmente se trata de un trade-off entre el uso de VRAM y la velocidad, excepto cuando el Data Loader en sí es el cuello de botella. Para acelerar el DataLoader, podemos pasar un argumento a num_workers para habilitar la paralelización en la preparación y carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f74f352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:05.406175Z",
     "start_time": "2023-08-06T06:30:02.556990Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de6b1867f45bd4673c2fe7c464049ae0",
     "grade": false,
     "grade_id": "cell-12cd6ba3b1e9f944",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the Word2VecDataset\n",
    "word2vec_dataset_ = Word2VecDataset(dataset['train'], vocab_size=n_v)\n",
    "\n",
    "# Convert the Word2VecDataset into a TensorDataset\n",
    "word2vec_dataset = TensorDataset(torch.tensor(word2vec_dataset_.data, dtype=torch.long))\n",
    "\n",
    "BATCH_SIZE = 2**16\n",
    "N_LOADER_PROCS = 5\n",
    "\n",
    "dataloader_train = DataLoader(word2vec_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=N_LOADER_PROCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53220df",
   "metadata": {},
   "source": [
    "### Paso 2 - Construyendo la Red\n",
    "La arquitectura que usaremos para esta ocasión será la dada por una versión de Word2Vec, esta consiste en:\n",
    "* Tres capas: Input, hidden y output\n",
    "* Tanto el tamaño de la input como la output son del tamaño del vocabulario. Pero la hidden es un poco más pequeña\n",
    "* Todas son Fully Connected con Funciones de Activación Lineales\n",
    "\n",
    "Como mencionamos en clase hay dos variantes\n",
    "* CBOW (Continuous Bag of Words): El enfoque está dado en las palabras de contexto para dar énfasis a la palabra central. O en otras palabras, las palabras de contexto son el input y la palabra central son el output (Espero que esto haga más sentido de la explicación en clase)\n",
    "* Skip-gram: La palabra central es el input, y las de contexto son la salida.\n",
    "\n",
    "Definamos CBOW para este laboratorio...\n",
    "\n",
    "Pero antes, debemos encodear nuestras palabras (otra vez como lo hicimos en el laboratorio pasado), esta implementación es similar a la que hicimos anteriormente, pero observen el uso de tensores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7927273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:05.419716Z",
     "start_time": "2023-08-06T06:30:05.412780Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79bcbc0e794ad778277522e74da6198f",
     "grade": false,
     "grade_id": "cell-719345b22d8a6412",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "size = 20\n",
    "input_ = 7\n",
    "\n",
    "def one_hot_encode(input_, size):\n",
    "    vec = torch.zeros(size).float()\n",
    "    # Aprox 1 linea para\n",
    "    # vec[input_] =\n",
    "    # YOUR CODE HERE\n",
    "    vec[input_] = 1.0\n",
    "    return vec\n",
    "\n",
    "ohe = one_hot_encode(input_, size)\n",
    "linear_layer = nn.Linear(size, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afda896e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:05.426787Z",
     "start_time": "2023-08-06T06:30:05.419716Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9076aa87b82f56230dfbf2cec1760ad",
     "grade": true,
     "grade_id": "cell-87b60412b0ba69d1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"3\"}--> \n",
       "         ✓ [3 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(3):        \n",
    "    assert int(ohe[7])  == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245325be",
   "metadata": {},
   "source": [
    "Ahora, sobreescribamos el comportamiento natural de la inicializacion de pesos, para que estos en lugar de iniciar aleatoriamente, sean valores de 0 - size. Esto lo hacemos dentro `torch.no_grad()` para quitar el tracking de la gradiente (recuerden que cuando usamos los tensores de PyTorch la gradiente se le hace tracking, es decir que se almacenan para hacer la diferenciar la pérdida con respecto de cada parametro en el modelo. Debido a que en esta ocasion lo estamos seteando manualmente no queremos que se almacene y sea considerado en futuras backpropagations.\n",
    "\n",
    "Observen como al pasar el vector encodeado a la capa nos devuelve efectivamente el número que corresponde en `linear_layer(ohe)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fbbf1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:05.440557Z",
     "start_time": "2023-08-06T06:30:05.426787Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d26f4b5566a553c62c142900b507a20",
     "grade": false,
     "grade_id": "cell-4c85a0ffbffa8e55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "         14., 15., 16., 17., 18., 19.]], requires_grad=True)\n",
      "tensor([7.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    linear_layer.weight = nn.Parameter(\n",
    "        torch.arange(size, dtype=torch.float).reshape(linear_layer.weight.shape))\n",
    "\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer(ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7ee35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9a663115c0df77e2c807969aac5ba8e",
     "grade": false,
     "grade_id": "cell-71d644e296aee563",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ya que tenemos un mejor entendimiento de este tipo de layers en Word2Vec, debemos saber que PyTorch tiene una implementación más eficiente usando `nn.Embedding`, el cual toma los índices de input y regresa el peso del borde correspondiente a ese índice.\n",
    "\n",
    "Un equivalente a lo que hemos hecho anteriormente sería lo que se presenta en la siguiente celda.\n",
    "\n",
    "Noten como volvemos a obtener un tensor similar al que obtuvimos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c53403aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:05.463215Z",
     "start_time": "2023-08-06T06:30:05.440557Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc320f7bb25be19e4e0033321279d77c",
     "grade": false,
     "grade_id": "cell-bf93477666a5691e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.],\n",
      "        [ 1.],\n",
      "        [ 2.],\n",
      "        [ 3.],\n",
      "        [ 4.],\n",
      "        [ 5.],\n",
      "        [ 6.],\n",
      "        [ 7.],\n",
      "        [ 8.],\n",
      "        [ 9.],\n",
      "        [10.],\n",
      "        [11.],\n",
      "        [12.],\n",
      "        [13.],\n",
      "        [14.],\n",
      "        [15.],\n",
      "        [16.],\n",
      "        [17.],\n",
      "        [18.],\n",
      "        [19.]], requires_grad=True)\n",
      "tensor([7.], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(size, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding_layer.weight = nn.Parameter(\n",
    "        torch.arange(size, dtype=torch.float\n",
    "        ).reshape(embedding_layer.weight.shape))\n",
    "\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor(input_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610c2f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bcc21a605eb207c161b41a6ded571ad",
     "grade": false,
     "grade_id": "cell-508a31edfe7f0cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Con esto en consideración, es momento de implementar nuestro modelo Word2Vec.\n",
    "\n",
    "Noten el embedding_size, este corresponde a la cantidad de representaciones de cada palabra, como dijimos en clase, esto sería la cantidad de funciones de activaciones con las que trabajaremos.\n",
    "\n",
    "Además, consideren las siguientes explicaciones\n",
    "\n",
    "`self.embed`: Es una capa de embedding para convertir la entrada (el índice del token de centro/contexto) en la codificación one-hot, y luego recuperar los pesos correspondientes a estos índices en la capa hidden de menor dimensión.\n",
    "\n",
    "`self.expand`: Es una capa lineal para predecir la probabilidad de una palabra de centro/contexto dada la hidden layer. Deshabilitamos el bias (la intercepción) porque cambiamos la escala de nuestras predicciones de todos modos.\n",
    "\n",
    "`logits`: Este vuelve a expandir la capa hidden para hacer predicciones. Estas predicciones sin procesar deben volver a escalarse con softmax, pero omitimos este paso aquí, ya que PyTorch implementa los pasos relevantes en la Cross Entropy loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "159c2e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T06:30:05.479149Z",
     "start_time": "2023-08-06T06:30:05.465210Z"
    }
   },
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        # Pasamos el input a una representación más pequeña\n",
    "        hidden = self.embed(input_)\n",
    "        # Expandemos hacia las predicciones\n",
    "        logits = self.expand(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b549e68",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2424728d269e4533722e107620b3f37",
     "grade": false,
     "grade_id": "cell-821b7f6886ebdd0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Paso 3 - Entrenamiento (Training)\n",
    "\n",
    "\n",
    "El entrenamiento en el contexto de las redes neuronales significa hacer predicciones repetidamente utilizando las observaciones en el conjunto de datos y luego ajustar los parámetros para corregir el error en las predicciones. \n",
    "\n",
    "Debido a que no queremos que la red aprenda perfectamente la predicción más reciente mientras olvida todas las demás predicciones, generalmente le damos un \"learning rate\", que es una penalización en el ajuste de pérdida para evitar que se ajuste solo a la observación más reciente. (Recuerden como funciona backpropgation)\n",
    "\n",
    "Cuanto más tiempo entrenemos la red, con mayor perfección aprenderá los datos de entrenamiento, pero a menudo esto conlleva el riesgo de overfitting y no poder generalizar a datos no vistos. Sin embargo, dado que con Word2Vec nuestro objetivo no es inferir datos no vistos, sino describir datos \"vistos\", ¿cuál creen que es la implicación del overfitting en este tipo de modelos? (Más adelante se deja nuevamente la pregunta para que sea respondida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b5522dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T22:14:29.330767Z",
     "start_time": "2023-08-06T22:14:29.296636Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a82d8c2a18f8548fa5a4b5765fb2fc1",
     "grade": false,
     "grade_id": "cell-588eec0490d68d93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "# Algunos hyper parametros\n",
    "\n",
    "# Demasiado pequeño pero es solo para fines de aprendizaje\n",
    "EMBED_SIZE = 50 \n",
    "model = Word2Vec(n_v, EMBED_SIZE)\n",
    "\n",
    "# Traten de usar ya el CUDA si pueden por favor\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using:\",device)\n",
    "model.to(device)\n",
    "\n",
    "# Otros parametros para el training\n",
    "LR = 3e-4\n",
    "EPOCHS = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Noten el tipo de optimizador que estamos usando :)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "784bfe0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T22:56:25.851356Z",
     "start_time": "2023-08-06T22:14:39.282185Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "469a6891ae3c2d3ef10bfe706471264f",
     "grade": false,
     "grade_id": "cell-8a89fd3a288c223a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with batch 0\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 1\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 2\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 3\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 4\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 5\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 6\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 7\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 8\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 9\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 10\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 11\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 12\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 13\n",
      "Done working with element 0\n",
      "Epoca 0, loss: 6.874203093591596\n",
      "Working with batch 0\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 1\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 2\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 3\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 4\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 5\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 6\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 7\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 8\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 9\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 10\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 11\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 12\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 13\n",
      "Done working with element 0\n",
      "Epoca 1, loss: 6.769147652242402\n",
      "Working with batch 0\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 1\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 2\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 3\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 4\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 5\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 6\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 7\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 8\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 9\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 10\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 11\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 12\n",
      "Done working with element 0\n",
      "Done working with element 6500\n",
      "Done working with element 13000\n",
      "Done working with element 19500\n",
      "Done working with element 26000\n",
      "Done working with element 32500\n",
      "Done working with element 39000\n",
      "Done working with element 45500\n",
      "Done working with element 52000\n",
      "Done working with element 58500\n",
      "Done working with element 65000\n",
      "Working with batch 13\n",
      "Done working with element 0\n",
      "Epoca 2, loss: 6.718393849577148\n"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    losses = []\n",
    "    for ix, batch in enumerate(dataloader_train):\n",
    "        print(f\"Working with batch {ix}\")\n",
    "        for i in range(len(batch[0])):\n",
    "            center = batch[0][i][0]\n",
    "            context = batch[0][i][1]\n",
    "            center, context = center.to(device), context.to(device)\n",
    "            # Aprox 1 linea para \n",
    "            # optimizer.zero...\n",
    "            # YOUR CODE HERE\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_=context)\n",
    "            # Aprox 1 linea para\n",
    "            # loss = loss_....\n",
    "            # YOUR CODE HERE\n",
    "            loss = loss_fn(logits, center)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i% 6500 == 0:\n",
    "                print(f\"Done working with element {i}\")\n",
    "            \n",
    "    epoch_loss = np.mean(losses)\n",
    "    running_loss.append(epoch_loss)\n",
    "    \n",
    "    # Mostrar la perdida cada N epocas\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoca {epoch}, loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe24f3e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T00:08:24.732102Z",
     "start_time": "2023-08-07T00:08:24.725620Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32ff5c07dd800c1cc3e4b2446f56cded",
     "grade": true,
     "grade_id": "cell-5fe835d2e4773764",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"5\"}--> \n",
       "         ✓ [5 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tick.marks(5):        \n",
    "    assert compare_numbers(new_representation(running_loss[len(running_loss)-1]), \"3c3d\", '0x1.b000000000000p+2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55ce97f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-06T23:47:02.588400Z",
     "start_time": "2023-08-06T23:47:02.385345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficamos la perdida\n",
    "epoch_ = np.arange(len(running_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch_, running_loss, 'r', label='Loss',)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a4424",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bead7c6fdad67968d9b943584a416c84",
     "grade": false,
     "grade_id": "cell-6ba5a104f61555b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Bueno, hemos visto la cantidad de tiemp que hay que invertirle para entrenar una red tan sencilla como la que se usa en Wor2Vec. En mi caso, usando CUDA le tomó alrededor de **42 minutos**. Ahora consideren aquel modelo donde no solo se sacan 50 representaciones de cada palabra sino miles, además que se entrenan por más epocas, no solo 3. \n",
    "\n",
    "Ahora veamos que tipo de palabras son las más cercanas a una pequeña muestra de 4 palabras. Para esto primero necesitamos sacar los pesos del modelo y pasarlos al cpu para trabajarlos como NumPy Arrays. Luego aplicaremos una función para encontrar la distancia dada una métrica (en este caso la distancia del coseno). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3acd42a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T00:38:03.418312Z",
     "start_time": "2023-08-07T00:38:03.384629Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adf71f64fc49e300a12791c23246a50b",
     "grade": false,
     "grade_id": "cell-79c39d4b13d3ed81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "wordvecs = model.expand.weight.cpu().detach().numpy()\n",
    "tokens = ['good', 'bad', 'school', 'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bae64a29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T00:38:23.499862Z",
     "start_time": "2023-08-07T00:38:23.360937Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e097e2559d99154cefeabf845d4aa0d",
     "grade": false,
     "grade_id": "cell-1d82d3393549def7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['feel', 'great', 'got', 'life', 'work', 'last', 'week', 'night', 'come', 'tomorrow'] \n",
      "\n",
      "bad ['news', 'made', 'better', 'thought', 'kill', 'fuck', 'lost', 'chang', 'could', 'man'] \n",
      "\n",
      "school ['busi', 'done', 'team', 'read', 'said', 'long', 'ill', 'order', 'oh', 'meet'] \n",
      "\n",
      "day ['amp', '@user', 'happi', 'go', 'get', 'love', 'today', 'im', 'like', 'make'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "def get_distance_matrix(wordvecs, metric):\n",
    "    dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
    "    return dist_matrix\n",
    "\n",
    "def get_k_similar_words(word, dist_matrix, k=10):\n",
    "    # Aprox 2 lineas para\n",
    "    # idx = ...\n",
    "    # dists = ... # Use la funcion dada arriba\n",
    "    # Hint: tok2id\n",
    "    # YOUR CODE HERE\n",
    "    idx = tok2id[word]\n",
    "    dists = dist_matrix[idx]\n",
    "    \n",
    "    ind = np.argpartition(dists, k)[:k+1]\n",
    "    ind = ind[np.argsort(dists[ind])][1:]\n",
    "    out = [(i, id2tok[i], dists[i]) for i in ind]\n",
    "    return out\n",
    "\n",
    "dmat = get_distance_matrix(wordvecs, 'cosine')\n",
    "for word in tokens:\n",
    "    print(word, [t[1] for t in get_k_similar_words(word, dmat)], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8137d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T00:38:47.402752Z",
     "start_time": "2023-08-07T00:38:47.384725Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "639c2eb3f9df4bd7244c8a482d492350",
     "grade": false,
     "grade_id": "cell-5e5ce486e8860c29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**PREGUNTAS:**\n",
    "* ¿Cuál es la implicación del overfitting en modelos como Word2Vec?\n",
    "\n",
    " Implica que el modelo ha aprendido demasiado los detalles específicos de los datos de entrenamiento y, por lo tanto, puede tener un rendimiento deficiente en datos nuevos o no vistos (conjunto de prueba) que no estaban presentes en el conjunto de entrenamiento. En el contexto de Word2Vec, el overfitting puede tener varias implicaciones como la reducción de la capacidad de generalización, la pérdida de información semántica o la sensibilidad a datos ruidosos.\n",
    "\n",
    "* ¿Qué tan bien encontró palabras cercanas su modelo Word2Vec? ¿Podría mejorar? ¿Cómo podría mejorar?\n",
    "\n",
    "Luego de haberse entrenado por más de 3 horas por haber uasdo cpu, se puede observar que de las palabras cercanas encontradas, de 10, 1 es bastante buena en casi todos los casos, pero el resto no tiene tanto sentido. Con un conjunto de palabras más grandes se podría mejorar el rendimiento del modelo, al igual que ajustando los hiperparámetros del modelo, como la dimensión del espacio vectorial, el tamaño del contexto de ventana, la tasa de aprendizaje y el número de épocas de entrenamiento.\n",
    "\n",
    "* A grandes rasgos, ¿cuál es la diferencia entre Word2Vec y BERT?\n",
    "\n",
    "Word2Vec es una herramienta para aprender representaciones de palabras, mientras que BERT es un modelo de lenguaje pre-entrenado que comprende de manera más profunda el contexto y se utiliza para mejorar el rendimiento en tareas de NLP mediante la transferencia de conocimiento pre-entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f5579",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a9b67df22196d0cba2dfbdcb310efc3",
     "grade": false,
     "grade_id": "cell-b8252dd53c19ab29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Parte 2 - Encoder - Decoder\n",
    "\n",
    "**Créditos:** La segunda parte de este laboratorio está tomado y basado en uno de los repositorios de Ben Trevett\n",
    "\n",
    "En esta ocasión vamos a centrarnos en una arquitectura Sequence to Sequence (Seq2Seq), entonces estaremos desarrollando un modelo que nos ayude a traducir de alemán a inglés. Tomaremos como base el paper [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215). Recuerden que a pesar que esto es para frases/oraciones, los conceptos pueden ser aplicados para otras arquitecturas similares.\n",
    "\n",
    "**IMPORTANTE:** Recuerden usar virtual enviroments debido a que estaremos usando versiones viejas de la librerías. ¿Por qué? Las librerías eran un poco más explícitas que sus versiones más recientes. A continuación se dejan los comandos para la instalación de las más importantes\n",
    "\n",
    "```\n",
    "pip install -U torch==1.9.0+cu111 -f  https://download.pytorch.org/whl/cu111/torch_stable.html\n",
    "pip install -U torchtext==0.10.0\n",
    "```\n",
    "\n",
    "El primer comando instalará la librería de PyTorch con CUDA 11.1\n",
    "El segundo, instala TorchText en una versión donde la formulación del vocabulario para training, test y validation era más claro (esta es la principal por la que estamos usando esta versiones).\n",
    "\n",
    "\n",
    "### Introducción\n",
    "Los modelos más comunes seq2seq son los modelos *encoder-decoder*, los cuales usan una RNN para encodear el input y llevarlo a un solo vector. En este laboratorio nos estaremos refiriendo a dicho vector como *vector contexto*. Pensemos sobre el vector contexto como un ser abstracto que representa una frase completa. Este vector es luego decodeado por una segunda RNN, que aprende a generar la frase target (output) deseada al generar palabra por palabra. \n",
    "\n",
    "\n",
    "Consideren la siguiente ilustración para representar el proceso que estaremos realizando\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/49df8404d938a6edbf729876405558cc2c2b3013/assets/seq2seq1.png\" alt=\"Seq2Seq\" />\n",
    "\n",
    "*Crédito de imagen al autor, imagen tomada de \"Sequence to Sequence Learning with Neural Networks\" de Ben Trevett*\n",
    "\n",
    "Noten como la frase de input \"guten morgen\", se pasa a través de una capa de embedding (cuadros amarillos) y luego entra en los encodeadores (cuadros verdes). En esta ocasión agregamos un token de \"start of sequence\" (`<sos>`) al inicio de la frase, además de un token de \"end of sequence\" (`<eos>`) al final de la oración. Vean como en cada paso, la entrada del encoder RNN es tanto la representación embedding $e$ de la palabra actual $e(x_t)$, así como el estado oculto del paso anterior $h_{t-1}$, y el encoder genera un nuevo hidden state $h_t$. Entonces, podemos pensar en el hidden state como una representación vectorial de la oración hasta ese momento. La RNN se puede representar como una función de tanto $e(x_t)$ y $h_{t-1}$\n",
    "\n",
    "$$h_t = \\text{EncoderRNN}(e(x_t), h_{t-1})$$\n",
    "\n",
    "Por favor noten que estamos usando el termino RNN de forma general en este contexto, puede ser cualquier arquitectura como LSTM o GRU.\n",
    "\n",
    "Entonces estaremos trabajando con una secuencia como $X = \\{x_1, x_2, ..., x_T\\}$, donde $x_1 = <sos>$, $x_2 = guten$, y así consecutivamente. El hidden state inicial $h_0$ es usualmente iniciado con ceros o con algún parametro pre-aprendido.\n",
    "\n",
    "Una vez la palabra final $X_T$ ha pasado en la RNN a través de la embedding layer, usamos el hidden state final $h_T$ como vector de contexto. Es decir, $h_T = z$. El cual será la representación vectorial de toda la oración.\n",
    "\n",
    "Ahora que tenemos nuestro vector de contexto $z$, podemos empezar a decodear para obtener la oración target, \"good morning\". De nuevo, agregamos los tokens de inicio y fin de la secuencia de nuestra oración target. En cada paso, el input al decoder RNN (cuadros azules de la imagen) es la versión embedding $d$ de la palabra actual $d(y_t)$ así como también el hidden state del paso previo $s_{t-1}$m donde el hidden state del decoder incial $s_0$ es el vector de contexto $s_0 = z = h_T$, es decir, el hidden state decoder es el último hidden state encoder. Por ende, simlar al encoder, podemos representarlo como: \n",
    "\n",
    "$$s_t = \\text{DecoderRNN}(d(y_t), s_{t-1})$$\n",
    "\n",
    "A pesar que el input embeeding layer $e$ y el target embedding layer $d$ están representados como cuadros amarillos en la imagen, como dijimos en clase, estas son dos embedding layers diferentes con sus propios parametros.\n",
    "\n",
    "En el decoder, necestamos ir del hidden state a la palabra actual, por ello en cada paso usamos $s_t$ para predecir (a traves de pasarlo en una layer lineal, mostrada como cuadros morados) lo que se cree que es la siguiente palabra en la secuencia $\\hat{y}_t$\n",
    "\n",
    "$$\\hat{y}_t = f(s_t)$$\n",
    "\n",
    "Las palabras en el decoder son siempre generadas una después de la otra, con una por paso. Siempre usamos `<sos>` para el primer input del decodr $y_1$ y algumas veces usamos la palabra predicha por nuestro decoder, $\\hat{y}_{t-1}$. Que, como mencionamos en clase, se le llama *teacher forcing*.\n",
    "\n",
    "Cuando estamos entrenando o probando nuestro modelo, siempre sabemso cuantas palabras hay en nuestra secuencia target, entonces nos detenemos de generar palabras una vez alcanzamos esa cantidad. Durante las fases de inferencia (uso del modelo en la \"vida real\") seguimos generando palabras hasta que el modelo genere un token `<eos>` o después de una cierta cantidad de palabras dada. (Esto tambien lo mencionamoos en clase, es solo para refrescar los conceptos) \n",
    "\n",
    "Una vez tengamos nuestra secuencia target predicha $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, la comparamos contra nuestra secuencia target real. $Y = \\{ y_1, y_2, ..., y_T \\}$, para calcular la perdida. Usamos esta pérdida para actualizar los parámetros del modelo, como bien hemos hecho en otras ocasiones.\n",
    "\n",
    "### Preparación de Data\n",
    "\n",
    "Es momento de ponernos a manos a la obra. Estaremos programando nuestro modelo usando PyTorch y usando torchtext para ayudarnos a hacer todo el pre-procesamiento necesario. Ahora usaremos spaCy para ayudarnos en la tokenización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b44ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.6\n",
      "  Using cached torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "Collecting tqdm (from torchtext==0.6)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.6 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 607.3 kB/s eta 0:00:00\n",
      "Collecting requests (from torchtext==0.6)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from torchtext==0.6) (1.9.0+cu111)\n",
      "Collecting numpy (from torchtext==0.6)\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/df/18/181fb40f03090c6fbd061bb8b1f4c32453f7c602b0dc7c08b307baca7cd7/numpy-1.25.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.25.2-cp39-cp39-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from torchtext==0.6) (1.16.0)\n",
      "Collecting sentencepiece (from torchtext==0.6)\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchtext==0.6)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/cb/dd/dce14328e6abe0f475e606131298b4c8f628abd62a4e6f27fdfa496b9efe/charset_normalizer-3.2.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp39-cp39-win_amd64.whl.metadata (31 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchtext==0.6)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchtext==0.6)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchtext==0.6)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from torch->torchtext==0.6) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from tqdm->torchtext==0.6) (0.4.6)\n",
      "Downloading numpy-1.25.2-cp39-cp39-win_amd64.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/15.6 MB 2.6 MB/s eta 0:00:06\n",
      "   ---------------------------------------- 0.2/15.6 MB 1.5 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.3/15.6 MB 2.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.5/15.6 MB 2.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/15.6 MB 3.6 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.1/15.6 MB 4.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.7/15.6 MB 5.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.3/15.6 MB 6.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.5/15.6 MB 5.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.8/15.6 MB 6.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/15.6 MB 6.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.0/15.6 MB 7.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.6/15.6 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.3/15.6 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.0/15.6 MB 8.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.5/15.6 MB 8.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.9/15.6 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.5/15.6 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 8.0/15.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.6/15.6 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.3/15.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.0/15.6 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.3/15.6 MB 9.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.7/15.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.1/15.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.6 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.9/15.6 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.4/15.6 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.8/15.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.3/15.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.5/15.6 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.1/15.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 62.6/62.6 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "   ---------------------------------------- 0.0/158.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 158.3/158.3 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.2.0-cp39-cp39-win_amd64.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.9/96.9 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "   ---------------------------------------- 0.0/123.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 123.9/123.9 kB 7.1 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, urllib3, tqdm, numpy, idna, charset-normalizer, certifi, requests, torchtext\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 idna-3.4 numpy-1.25.2 requests-2.31.0 sentencepiece-0.1.99 torchtext-0.6.0 tqdm-4.66.1 urllib3-2.0.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U torch==1.9.0+cu111 -f  https://download.pytorch.org/whl/cu111/torch_stable.html\n",
    "#!pip install -U torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068e0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/61/08/f21d6f07a879cdfe284bc5bacfcf86c054866c24fe2e7c2e383d7a04421b/spacy-3.6.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.6.1-cp39-cp39-win_amd64.whl.metadata (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 0.0/96.8 kB ? eta -:--:--\n",
      "     ---------------- ----------------------- 41.0/96.8 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 96.8/96.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.2.0,>=8.1.8 from https://files.pythonhosted.org/packages/68/89/532e8b45cd40ba92faf81e1089edfc0181b060dd3027f09681284b774ccd/thinc-8.1.12-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.1.12-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/a3/f3/31643982ebae1ed9458f433a56ab718607b22d779ebd11144d8fa521281b/srsly-2.4.7-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.7-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/45/8f/5b73efc14e0373d9bb0de6ce1ab04a8f77420dc473f1f3ef270caf085cff/catalogue-2.0.9-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.9/45.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.10.0 (from spacy)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.8/56.8 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/87/80/52770e747e4bee5012e60b2684db36c8fdf010f8dadb4ded0efec808b07d/pydantic-2.1.1-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl.metadata (136 kB)\n",
      "     ---------------------------------------- 0.0/136.5 kB ? eta -:--:--\n",
      "     -------------------------------------  133.1/136.5 kB 7.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 136.5/136.5 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 181.6/181.6 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/d8/f0/a2ee543a96cc624c35a9086f39b1ed2aa403c6d355dfe47a11ee5c64a164/annotated_types-0.5.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic-core==2.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for pydantic-core==2.4.0 from https://files.pythonhosted.org/packages/6f/29/6cb9e3c92c39a2fae99b538489c0f37c846ad756c9e8455782f0a2466478/pydantic_core-2.4.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.4.0-cp39-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/a1/cb/5b69b82b8a1eafc3056d2f0b3f04eb7426fb894436df8ef4afbe8360efa6/blis-0.7.10-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.10-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/05/5e/1a56e81e3335ce18f6742539edd2f6ea98bbf477fefc9a4dc5c0694bace4/confection-0.1.1-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for click<9.0.0,>=7.1.1 from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/a2/b2/624042cb58cc6b3529a6c3a7b7d230766e3ecb768cba118ba7befd18ed6f/MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Downloading spacy-3.6.1-cp39-cp39-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/12.1 MB 11.2 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.4/12.1 MB 11.2 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/12.1 MB 5.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.1 MB 6.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 5.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.1/12.1 MB 7.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/12.1 MB 7.3 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.5/12.1 MB 7.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.8/12.1 MB 6.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 7.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.1 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.4/12.1 MB 7.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.8/12.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.8/12.1 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 7.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.4/12.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.0/12.1 MB 7.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.5/12.1 MB 7.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.1 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.1 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.1 MB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.1 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/12.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "   ---------------------------------------- 0.0/48.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.9/48.9 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.9 kB ? eta -:--:--\n",
      "   --------------------------------------  368.6/370.9 kB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 370.9/370.9 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.4.0-cp39-none-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.6/1.7 MB 17.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.1/1.7 MB 14.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading srsly-2.4.7-cp39-cp39-win_amd64.whl (483 kB)\n",
      "   ---------------------------------------- 0.0/483.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 483.6/483.6 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.1.12-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.4/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Downloading blis-0.7.10-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "   ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.4 MB 16.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.9/7.4 MB 11.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.4/7.4 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.0/7.4 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.7/7.4 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.4/7.4 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.0/7.4 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.7/7.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.2/7.4 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.4/7.4 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.6/7.4 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.8/7.4 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.0/7.4 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.2/7.4 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.4/7.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.6/7.4 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.8/7.4 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.2/7.4 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.4/7.4 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Downloading MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, MarkupSafe, langcodes, click, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, jinja2, pathy, confection, thinc, spacy\n",
      "Successfully installed MarkupSafe-2.1.3 annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 click-8.1.6 confection-0.1.1 cymem-2.0.7 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-2.1.1 pydantic-core-2.4.0 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.12 typer-0.9.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9194c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# URLs de los conjuntos de datos\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n",
    "    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\",\n",
    "]\n",
    "\n",
    "# Función para descargar y descomprimir el archivo\n",
    "def download_and_extract(url, extract_path='C:\\\\Users\\\\andre\\\\Downloads\\\\Training'):\n",
    "    response = requests.get(url, stream=True)\n",
    "    tar_file_path = os.path.join(extract_path, url.split(\"/\")[-1])\n",
    "    with open(tar_file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=128):\n",
    "            file.write(chunk)\n",
    "    with tarfile.open(tar_file_path, 'r:gz') as file:\n",
    "        file.extractall(path=extract_path)\n",
    "    \n",
    "    #for filename in glob.glob(os.path.join(extract_path, '*.de')) + glob.glob(os.path.join(extract_path, '*.en')):\n",
    "        #os.rename(filename, filename[:-3] + '2016' + filename[-3:])\n",
    "    os.remove(tar_file_path) # Elimina el archivo tar.gz después de la extracción\n",
    "\n",
    "\n",
    "# Descargar y descomprimir los archivos\n",
    "for url in urls:\n",
    "     download_and_extract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b5246f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:04.622634Z",
     "start_time": "2023-08-07T12:09:55.185815Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "val_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "test_url = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "\n",
    "# Update the URLs in the Multi30k module\n",
    "Multi30k.urls = (train_url, val_url, test_url)\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f19f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb193611b74e155202745762d0e05a22",
     "grade": false,
     "grade_id": "cell-960a94af6dbde6f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Colocamos las semillas para tener resultados consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2182c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:12.966554Z",
     "start_time": "2023-08-07T12:10:12.934552Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85333315",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32e4c72dc8e544489e4bb9de559fc363",
     "grade": false,
     "grade_id": "cell-1a4d13aff19cbbc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora, necesitamos crear un tokenizador. Un tokenizador es una función que ayudará a convertir un string de alguna frase u oración en una lista de tokens individuales. Consideren que en una frase como \"good morning!\" se tienen tres tokens, siendo cada uno \"good\", \"morning\" y \"!\", noten que a pesar que el signo de admiracion no se considera una palabra, sí se considera como un token. \n",
    "\n",
    "Para la creación de nuestro tokenizador nos apoyaremos en spaCy, en este caso necesitamos los paquetes de aleman e inglés (se nombran abajo).\n",
    "\n",
    "Para instalar spaCy necesitarán ejecutar en la cmd \n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "**IMPORTANTE:** Recuerden usar virtual environments de Python, debido a que este laboratorio usa algunas librerías deprecadas, que como se explicó previamente, se hizo de este modo para ser más explícito el aprendizaje.\n",
    "\n",
    "Regresando al tema del tokenizer, primero cargaremos las dos versiones para los diferentes idiomas con los que estamos trabajando.\n",
    "\n",
    "Despues, crearemos unas funciones de tokenización. Estas pueden ser pasadas a TorchText y tomarán una oración y regresara la oración como una lista de tokens.\n",
    "\n",
    "Cabe la pena mencionar que en el paper que estamos tomando de base, ellos encontrarón util el revertir el orden del input dado que se cree que introducía varias dependencias a corto plazo en los datos que facilitan mucho el problema de optimización.\n",
    "\n",
    "Más adelante, usaremos `Field` (que actualmente está deprecado :( ) para manejar como la data debería ser procesada. Después, seteamos el parametro `tokenize` como función para cada caso. El aleman será el `SRC` y el inglés será el `TRG`. Además también se agrega el token para inicio y fin de la secuencia, además que convertirá todo en lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3212c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 682.7 kB/s eta 0:00:19\n",
      "     --------------------------------------- 0.0/12.8 MB 495.5 kB/s eta 0:00:26\n",
      "     --------------------------------------- 0.1/12.8 MB 573.4 kB/s eta 0:00:23\n",
      "     --------------------------------------- 0.1/12.8 MB 654.9 kB/s eta 0:00:20\n",
      "      -------------------------------------- 0.2/12.8 MB 892.5 kB/s eta 0:00:15\n",
      "      --------------------------------------- 0.3/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.5 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 1.9/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 5.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 5.7 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.1/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 5.8 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.9/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
      "     ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/14.6 MB 660.6 kB/s eta 0:00:23\n",
      "     --------------------------------------- 0.0/14.6 MB 393.8 kB/s eta 0:00:38\n",
      "     --------------------------------------- 0.1/14.6 MB 655.4 kB/s eta 0:00:23\n",
      "     --------------------------------------- 0.2/14.6 MB 833.5 kB/s eta 0:00:18\n",
      "      --------------------------------------- 0.2/14.6 MB 1.0 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 0.4/14.6 MB 1.4 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/14.6 MB 1.6 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.7/14.6 MB 2.0 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 1.1/14.6 MB 2.6 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.2/14.6 MB 2.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.5/14.6 MB 3.1 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.9/14.6 MB 3.5 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 2.2/14.6 MB 3.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.5/14.6 MB 3.9 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.8/14.6 MB 4.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.9/14.6 MB 3.9 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 3.0/14.6 MB 3.9 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 3.3/14.6 MB 3.9 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.8/14.6 MB 4.3 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 4.4/14.6 MB 4.7 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 5.0/14.6 MB 5.1 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 5.4/14.6 MB 5.3 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.9/14.6 MB 5.6 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 6.5/14.6 MB 5.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 7.0/14.6 MB 6.1 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 7.6/14.6 MB 6.3 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.3/14.6 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.9/14.6 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.4/14.6 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 9.9/14.6 MB 7.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 10.4/14.6 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 10.9/14.6 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 11.4/14.6 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 12.1/14.6 MB 9.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 12.6/14.6 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 13.1/14.6 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 13.6/14.6 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 14.1/14.6 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.6 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.6/14.6 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\andre\\onedrive\\documentos\\github\\lab4_deeplearning\\.venv\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4bc4d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:15.907822Z",
     "start_time": "2023-08-07T12:10:14.202661Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e7543a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:16.426717Z",
     "start_time": "2023-08-07T12:10:16.410680Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1227f75a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:16.775400Z",
     "start_time": "2023-08-07T12:10:16.767392Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fd2f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4c7c0410bb12c03fd3a415d086f6aa1",
     "grade": false,
     "grade_id": "cell-f059a0654bba3e20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora, debemos descargar el dataset. Para este caso estaremos usando el dataset llamado Multi30k. Este tiene aproximadamente 30K frases en inglés, aleman y francés, cada uno tiene alrededor de 12 palabras por frase. \n",
    "\n",
    "Además noten que `exts` especifica cual lenguage se debe usar como source y target, y `fields` da cuales campos usar para el source y target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c2f3811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:21.558532Z",
     "start_time": "2023-08-07T12:10:17.137695Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG),\n",
    "                                                    path = 'C:\\\\Users\\\\andre\\\\Downloads\\\\Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9daf8a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:21.990868Z",
     "start_time": "2023-08-07T12:10:21.982854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de observaciones de training: 29000\n",
      "Numero de observaciones en validation: 1014\n",
      "Numero de observaciones en test: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero de observaciones de training: {len(train_data.examples)}\")\n",
    "print(f\"Numero de observaciones en validation: {len(valid_data.examples)}\")\n",
    "print(f\"Numero de observaciones en test: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56422a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:23.001453Z",
     "start_time": "2023-08-07T12:10:22.753770Z"
    }
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af8e7be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:22.369708Z",
     "start_time": "2023-08-07T12:10:22.353942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e20d7",
   "metadata": {},
   "source": [
    "Observen como el punto está al comienzo de la oración en alemán (src), por lo que parece que la oración se invirtió correctamente.\n",
    "\n",
    "Ahora, construiremos el vocabulario para los idiomas de source y de target. El vocabulario se utiliza para asociar cada token único con un índice (un número entero). Los vocabularios de los idiomas de origen y de destino son distintos.\n",
    "\n",
    "Usando el argumento `min_freq`, solo permitimos que aparezcan en nuestro vocabulario tokens que aparecen al menos 2 veces. Los tokens que aparecen solo una vez se convierten en un token desconocido `<unk>`.\n",
    "\n",
    "Es importante tener en cuenta que nuestro vocabulario solo debe construirse a partir del conjunto de entrenamiento y no del conjunto de validación/test. Esto evita la \"fuga de información\" en nuestro modelo, dándonos puntajes de validación/prueba inflados artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b865dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:23.312139Z",
     "start_time": "2023-08-07T12:10:23.299580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7853\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686bcfc",
   "metadata": {},
   "source": [
    "El paso final de preparar los datos es crear los iteradores. Estos se pueden iterar para devolver un lote de datos que tendrá un atributo `src` (los tensores de PyTorch \n",
    "que contienen un lote de oraciones de origen numeradas) y un atributo `trg` (los tensores de PyTorch que contienen un batch de oraciones de destino numeradas). \n",
    "\"Numericalized\" es solo una forma elegante de decir que se han convertido de una secuencia de tokens legibles a una secuencia de índices correspondientes, usando el vocabulario.\n",
    "\n",
    "También necesitamos definir un dispositivo `torch.device`. Esto se usa para indicarle a torchText que coloque o no los tensores en la GPU. \n",
    "Usamos la función `torch.cuda.is_available()`, que devolverá True si se detecta una GPU en nuestra computadora. Pasamos este dispositivo al iterador.\n",
    "\n",
    "Cuando obtenemos un lote de ejemplos usando un iterador, debemos asegurarnos de que todas las oraciones de origen tengan la misma longitud, \n",
    "al igual que las oraciones de destino. ¡Afortunadamente, los iteradores de torchText manejan esto por nosotros!\n",
    "\n",
    "Usamos un `BucketIterator` en lugar del `Iterador` estándar, ya que crea lotes de tal manera que minimiza la cantidad de padding en las oraciones de origen y de destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6fbb387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:23.716604Z",
     "start_time": "2023-08-07T12:10:23.695697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa44707d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:29.661585Z",
     "start_time": "2023-08-07T12:10:29.630173Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725df266",
   "metadata": {},
   "source": [
    "### Construyendo el Modelo Seq2Seq\n",
    "Vamos a definir nuestro modelo en tres partes, el encoder, el decoder y el modelo Seq2Seq. Este ultimo encapsulará el proceso y transferencia entre los primeros dos.\n",
    "\n",
    "#### Encoder\n",
    "Primero, el encoder, es un LSTM de 2 capas. El paper que estamos implementando usa un LSTM de 4 capas, pero en favor del tiempo de entrenamiento lo reducimos a 2 capas. \n",
    "El concepto de RNN multicapa es fácil de expandir de 2 a 4 capas.\n",
    "\n",
    "Para un RNN multicapa, la oración de entrada, $X$, después de ser embeddida va a la primera capa (inferior) del RNN y los estados ocultos, $H=\\{h_1, h_2, ..., h_T\\}$ , \n",
    "la salida de esta capa se utiliza como entrada a la RNN en la capa superior. Así, representando cada capa con un superíndice, los hidden states en la primera capa vienen dados por:\n",
    "\n",
    "$$h_t^1 = \\text{EncoderRNN}^1(e(x_t), h_{t-1}^1)$$\n",
    "\n",
    "Las hidden states en la segunda layer son dadas por:\n",
    "\n",
    "$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$\n",
    "\n",
    "El uso de un RNN multicapa también significa que también necesitaremos un hidden state inicial como entrada por capa, $h_0^l$, y también generaremos un vector de contexto por capa, $z^l$.\n",
    "\n",
    "Si desean repasar un poco sobre LSTM pueden consultar este [enlance] (https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "Para este laboratorio, es suficiente que recuerden que lo que necesitamos saber es los LSTM, en lugar de simplemente tomar un estado oculto y devolver un nuevo estado oculto por paso de tiempo, \n",
    "también toman y devuelven un *estado de celda*, $c_t$, por paso de tiempo.\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t &= \\text{RNN}(e(x_t), h_{t-1})\\\\\n",
    "(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\n",
    "\\end{align*}$$\n",
    "\n",
    "Podemos pensar en $c_t$ como otro tipo de hidden state. Similar a $h_0^l$, $c_0^l$ se inicializará en un tensor de ceros. \n",
    "Además, nuestro vector de contexto ahora será tanto el hidden state final como el estado de celda final, es decir, $z^l = (h_T^l, c_T^l)$.\n",
    "\n",
    "Al extender nuestras ecuaciones multicapa a LSTM, obtenemos:\n",
    "\n",
    "$$\\begin{align*}\n",
    "(h_t^1, c_t^1) &= \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(h_t^2, c_t^2) &= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Observen cómo solo nuestro hidden state de la primera capa se pasa como entrada a la segunda capa, y no el estado de la celda.\n",
    "\n",
    "Así que nuestro codificador se parece a esto:\n",
    "\n",
    "IMAGEN\n",
    "\n",
    "Creamos esto en el código creando un módulo `Encoder`, que requiere que heredemos de `torch.nn.Module` y usemos `super().__init__()` como un código repetitivo. \n",
    "El codificador toma los siguientes argumentos:\n",
    "- `input_dim` es el tamaño/dimensionalidad de los vectores one-hot que se ingresarán al codificador. Esto es igual al tamaño del vocabulario de entrada (fuente).\n",
    "- `emb_dim` es la dimensionalidad de la capa de embedding. Esta capa convierte los vectores one-hot en vectores densos con dimensiones `emb_dim`.\n",
    "- `hid_dim` es la dimensionalidad de los estados ocultos y de celda.\n",
    "- `n_layers` es el número de capas en el RNN.\n",
    "- `dropout` es la cantidad de abandono a utilizar. Este es un parámetro de regularización para evitar el overfitting. \n",
    "Consulte [aqui] (https://www.coursera.org/lecture/deep-neural-network/understanding-dropout-YaGbR) para obtener más detalles sobre dropout.\n",
    "\n",
    "\n",
    "No vamos a discutir la capa de embedding en detalle durante aqui pues ya lo hicimos previamente. Todo lo que necesitamos saber es que hay un paso antes de que las palabras \n",
    "(técnicamente, los índices de las palabras) pasen al RNN, donde las palabras se transforman en vectores. Para leer más sobre embedding de palabras, \n",
    "consulten estos artículos: [1](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/), [2](http://p.migdal.pl /2017/01/06/rey-hombre-mujer-reina-por qué.html), [3](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ ), [4](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/).\n",
    "\n",
    "La capa de embedding se crea usando `nn.Embedding`, el LSTM con `nn.LSTM` y una capa de dropout con `nn.Dropout`. \n",
    "Consulten la [documentación de PyTorch ] (https://pytorch.org/docs/stable/nn.html) para obtener más información al respecto.\n",
    "\n",
    "Una cosa a tener en cuenta es que el argumento `dropout` para el LSTM es cuánto dropout aplicar entre las capas de un RNN multicapa, \n",
    "es decir, entre la salida de estados ocultos de la capa $l$ y esos mismos estados ocultos que se utilizan para el entrada de la capa $l+1$.\n",
    "\n",
    "En el método `forward`, pasamos la oración fuente, $X$, que se convierte en vectores densos usando la capa `embedding`, y luego se aplica el dropout. \n",
    "Estos embedding luego se pasan a la RNN. A medida que pasamos una secuencia completa a la RNN, ¡automáticamente hará el cálculo recurrente de los estados \n",
    "ocultos en toda la secuencia por nosotros! Tenga en cuenta que no pasamos un estado inicial oculto o de celda al RNN. \n",
    "Esto se debe a que, como se indica en la [documentación](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM), si no se pasa ningún estado de celda/oculto a la RNN, crea automáticamente un estado inicial de celda/oculto como un tensor de ceros.\n",
    "\n",
    "El RNN devuelve: `outputs` (el hidden state de la capa superior para cada paso de tiempo), `hidden` (el hidden state final para cada capa, $h_T$, \n",
    "apiladas una encima de la otra) y `cell` (la estado de celda final para cada capa, $c_T$, apilados uno encima del otro).\n",
    "\n",
    "Como solo necesitamos los hidden state y de celda finales (para hacer nuestro vector de contexto), `forward` solo devuelve `hidden` y `cell`.\n",
    "\n",
    "Los tamaños de cada uno de los tensores se dejan como comentarios en el código. En esta implementación, `n_directions` siempre será 1, sin embargo, \n",
    "tengan en cuenta que los RNN bidireccionales (cubiertos en el tutorial 3) tendrán `n_directions` como 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b79c7b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:30.685603Z",
     "start_time": "2023-08-07T12:10:30.659165Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24a7fe486fef7641af76e0308c8553e5",
     "grade": false,
     "grade_id": "cell-819cfe4960d74aaf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85acaa4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9c175e73457d779955f7a23deb52244",
     "grade": false,
     "grade_id": "cell-5f5382d652012307",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Decoder\n",
    "Ahora pasaremos a construir el decoder, el cual también será una 2-layer (4 en el paper) LSTM.\n",
    "\n",
    "![](assets/seq2seq3.png)\n",
    "\n",
    "\n",
    "La clase `Decoder` hace un solo paso de decodificación, es decir, genera un solo token por paso. La primera capa recibirá un hidden state y de celda del paso de tiempo anterior,\n",
    "$(s_{t-1}^1, c_{t-1}^1)$, y lo alimenta a través del LSTM con el token incrustado actual, $y_t$, para producir un nuevo hidden state y de celda, $(s_t ^1, c_t^1)$.\n",
    "Las capas subsiguientes usarán el estado oculto de la capa inferior, $s_t^{l-1}$, y los estados ocultos y de celda anteriores de su capa, $(s_{t-1}^l, c_{t-1) }^l)$.\n",
    "Esto proporciona ecuaciones muy similares a las del codificador.\n",
    "\n",
    "$$\\begin{align*}\n",
    "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Recuerde que los estados iniciales ocultos y de celda de nuestro decoder son nuestros vectores de contexto, que son los estados finales ocultos y de celda de nuestro decoder de la misma capa,\n",
    "es decir, $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
    "\n",
    "Luego pasamos el hidden state desde la capa superior del RNN, $s_t^L$, a través de una capa lineal, $f$, para hacer una predicción de cuál será el siguiente token en la secuencia de destino (salida).\n",
    "debería ser, $\\hat{y}_{t+1}$.\n",
    "\n",
    "$$\\sombrero{y}_{t+1} = f(s_t^L)$$\n",
    "\n",
    "Los argumentos y la inicialización son similares a la clase `Encoder`, excepto que ahora tenemos un `output_dim` que es el tamaño del vocabulario para la salida/objetivo.\n",
    "También está la adición de la capa 'Lineal', utilizada para hacer las predicciones desde el hidden state de la capa superior.\n",
    "\n",
    "Dentro del método `forward`, aceptamos un batch de tokens de entrada, hidden state anteriores y estados de celda anteriores. Como solo estamos decodificando un token a la vez,\n",
    "los tokens de entrada siempre tendrán una longitud de secuencia de 1. \"Aflojamos\" los tokens de entrada para agregar una dimensión de longitud de oración de 1. Luego, de forma similar al encoder,\n",
    "pasamos a través de una capa de embedding y aplicamos dropout. Este batch de tokens embeddidos luego se pasa al RNN con los estados ocultos y de celda anteriores.\n",
    "Esto produce una \"salida\" (hidden state de la capa superior de la RNN), un nuevo \"hidden state\" (uno para cada capa, apilados uno encima del otro) y una nueva \"celda\".\n",
    "estado (también uno por capa, apilados uno encima del otro). Luego pasamos la `salida` (después de deshacernos de la dimensión de longitud de la oración) a través de la capa lineal para recibir nuestra\n",
    "`predicción`. Luego devolvemos la `predicción`, el nuevo hidden state y el nuevo estado `celular`.\n",
    "\n",
    "**Nota**: como siempre tenemos una longitud de secuencia de 1, podríamos usar `nn.LSTMCell`, en lugar de `nn.LSTM`, ya que está diseñado para manejar un lote de entradas que no son\n",
    "necesariamente en una secuencia. `nn.LSTMCell` es solo una sola celda y `nn.LSTM` es un envoltorio alrededor de múltiples celdas potenciales. Usando `nn.LSTMCell` en este caso\n",
    "significaría que no tenemos que `descomprimir` para agregar una dimensión de longitud de secuencia falsa, pero necesitaríamos un `nn.LSTMCell` por capa en el decoder y para asegurar que cada `nn.LSTMCell`\n",
    "recibe el hidden state inicial correcto del codificador. Todo esto hace que el código sea menos conciso, de ahí la decisión de seguir con el `nn.LSTM` regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caa53fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:33.673863Z",
     "start_time": "2023-08-07T12:10:33.642635Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9062a639f1c3bd604869ed020a65ea7e",
     "grade": false,
     "grade_id": "cell-84131f43444e74fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2746420",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "\n",
    "Para la parte final de la implementación, implementaremos el modelo seq2seq. Esto manejará:\n",
    "- recibir la oración de entrada/fuente\n",
    "- usar el encoder para producir los vectores de contexto\n",
    "- usar el decoder para producir la salida predicha/oración objetivo\n",
    "\n",
    "Nuestro modelo completo se verá así:\n",
    "\n",
    "![](activos/seq2seq4.png)\n",
    "\n",
    "El modelo `Seq2Seq` incluye un `Encoder`, un `Decoder` y un `dispositivo` (usado para colocar tensores en la GPU, si existe).\n",
    "\n",
    "Para esta implementación, debemos asegurarnos de que el número de capas y las dimensiones ocultas (y de celda) sean iguales en el 'Encoder' y 'Decoder'.\n",
    "Este no es siempre el caso, no necesariamente necesitamos la misma cantidad de capas o los mismos tamaños de dimensiones ocultas en un modelo de sequence to sequence.\n",
    "Sin embargo, si hiciéramos algo como tener un número diferente de capas, tendríamos que tomar decisiones sobre cómo manejar esto.\n",
    "Por ejemplo, si nuestro encoder tiene 2 capas y nuestro decoder solo tiene 1, ¿cómo se maneja esto? ¿Promediamos los dos vectores de contexto generados por el decoder?\n",
    "¿Pasamos ambos por una capa lineal? ¿Solo usamos el vector de contexto de la capa más alta? Etc.\n",
    "\n",
    "Nuestro método \"forward\" toma la oración fuente, la oración objetivo y un ratio de teacher-forcing. El ratio de teacher-forcing se usa cuando entrenamos nuestro modelo.\n",
    "Al decodificar, en cada paso, predeciremos cuál será el próximo token en la secuencia de destino de los tokens anteriores decodificados, $\\hat{y}_{t+1}=f(s_t^L)$. \n",
    "Con una probabilidad igual a la tasa de teacher forcing (`teacher_forcing_ratio`), utilizaremos el siguiente token real de la secuencia como entrada al decoder durante el siguiente paso.\n",
    "Sin embargo, con probabilidad `1 - Teacher_forcing_ratio`, usaremos el token que el modelo predijo como la próxima entrada al modelo, incluso si no coincide con el siguiente token real en la secuencia.\n",
    "\n",
    "Lo primero que hacemos en el método `forward` es crear un tensor `outputs` que almacenará todas nuestras predicciones, $\\hat{Y}$.\n",
    "\n",
    "Luego alimentamos la oración de entrada/fuente, `src`, en el encoder y recibimos los estados ocultos y de celda finales.\n",
    "\n",
    "La primera entrada al decoder es el token de inicio de secuencia (`<sos>`). Como nuestro tensor `trg` ya tiene el token `<sos>` agregado (desde cuando definimos el `init_token` en nuestro campo `TRG`)\n",
    "obtenemos nuestro $y_1$ cortándolo. Sabemos qué tan largas deben ser nuestras oraciones de destino (`max_len`), por lo que las repetimos muchas veces. El último token ingresado en el decoder es el **antes** del token `<eos>` - el `<eos>`\n",
    "el token nunca se ingresa en el decoder.\n",
    "\n",
    "Durante cada iteración del ciclo, nosotros:\n",
    "- pasar la entrada, los estados de celda anteriores ocultos y anteriores ($y_t, s_{t-1}, c_{t-1}$) al decoder\n",
    "- recibir una predicción, el siguiente estado oculto y el siguiente estado de celda ($\\hat{y}_{t+1}, s_{t}, c_{t}$) del decoder\n",
    "- colocar nuestra predicción, $\\hat{y}_{t+1}$/`output` en nuestro tensor de predicciones, $\\hat{Y}$/`outputs`\n",
    "- decidir si vamos a \"fuerza de maestros\" o no\n",
    "     - si lo hacemos, la siguiente 'entrada' es el siguiente token de verdad fundamental en la secuencia, $y_{t+1}$/`trg[t]`\n",
    "     - si no lo hacemos, la siguiente `entrada` es el siguiente token predicho en la secuencia, $\\hat{y}_{t+1}$/`top1`, que obtenemos al hacer un `argmax` sobre el tensor de salida\n",
    "    \n",
    "Una vez que hemos hecho todas nuestras predicciones, devolvemos nuestro tensor lleno de predicciones, $\\hat{Y}$/`outputs`.\n",
    "\n",
    "**Nota**: nuestro ccilo decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de `salidas` sigue siendo todo ceros. Así que nuestras `trg` y `outputs` se parecen a:\n",
    "\n",
    "$$\\begin{alinear*}\n",
    "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{resultados} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "Posteriormente cuando calculamos la pérdida, cortamos el primer elemento de cada tensor para obtener:\n",
    "\n",
    "$$\\begin{alinear*}\n",
    "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{salidas} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f50e76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:34.696215Z",
     "start_time": "2023-08-07T12:10:34.680165Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a053dcb6ba362103fad11691a8c9cdfd",
     "grade": false,
     "grade_id": "cell-3cf708a546f162a5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            \n",
    "            # Aprox 1 linea para \n",
    "            # output, hidden, cell =\n",
    "            # YOUR CODE HERE\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6723282",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "206f7ba1031ec9e31fc458fbe10ce7a4",
     "grade": false,
     "grade_id": "cell-8024abbda3c1898a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training Seq2Seq Model\n",
    "\n",
    "Ahora que tenemos nuestro modelo implementado, podemos comenzar a entrenarlo.\n",
    "\n",
    "Primero, inicializaremos nuestro modelo. Como se mencionó anteriormente, las dimensiones de entrada y salida están definidas por el tamaño del vocabulario.\n",
    "Las dimensiones de embedding y el dropout del encoder y el decoder pueden ser diferentes, pero el número de capas y el tamaño de los estados ocultos/de celda deben ser los mismos.\n",
    "\n",
    "Luego definimos el encoder, el decoder y luego nuestro modelo Seq2Seq, que colocamos en el \"device\".\n",
    "\n",
    "\n",
    "El siguiente paso es inicializar los pesos de nuestro modelo. En el paper afirman que inicializan todos los pesos a partir de una distribución uniforme entre -0,08 y +0,08, es decir, $\\mathcal{U}(-0,08, 0,08)$.\n",
    "\n",
    "Inicializamos los pesos en PyTorch creando una función que \"aplicamos\" a nuestro modelo. Al usar `apply`, se llamará a la función `init_weights` en cada módulo y submódulo dentro de nuestro modelo.\n",
    "Para cada módulo, recorremos todos los parámetros y los muestreamos desde una distribución uniforme con `nn.init.uniform_`.\n",
    "\n",
    "\n",
    "También definimos una función que calculará el número de parámetros entrenables en el modelo.\n",
    "\n",
    "\n",
    "Definimos nuestro optimizador, que usamos para actualizar nuestros parámetros en el ciclo de entrenamiento. Consulte [esta publicación](http://ruder.io/optimizing-gradient-descent/) \n",
    "para obtener información sobre diferentes optimizadores. Aquí usaremos a Adam\n",
    "\n",
    "A continuación, definimos nuestra función de pérdida. La función `CrossEntropyLoss` calcula tanto el log softmax como la log-likelihood negativo de nuestras predicciones.\n",
    "\n",
    "Nuestra función de pérdida calcula la pérdida promedio por token, sin embargo, al pasar el índice del token `<pad>` como el argumento `ignore_index`, ignoramos la pérdida siempre que el token de destino sea un token de relleno (padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8eac57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:41.167607Z",
     "start_time": "2023-08-07T12:10:35.024487Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04cb6773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:41.857202Z",
     "start_time": "2023-08-07T12:10:41.472920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d5fffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:42.370872Z",
     "start_time": "2023-08-07T12:10:42.339409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 13,898,501 parametros entrenables\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'El modelo tiene {count_parameters(model):,} parametros entrenables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c927da4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:42.863269Z",
     "start_time": "2023-08-07T12:10:42.847270Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1655eb2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:43.263371Z",
     "start_time": "2023-08-07T12:10:43.247371Z"
    }
   },
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae5ea8",
   "metadata": {},
   "source": [
    "A continuación, definiremos nuestro ciclo de entrenamiento.\n",
    "\n",
    "Primero, configuraremos el modelo en \"modo de entrenamiento\" con `model.train()`. Esto activará el dropout (y batch normalization, que no estamos usando) y luego iterará a través de nuestro iterador de datos.\n",
    "\n",
    "Como se indicó anteriormente, nuestro ciclo decodificador comienza en 1, no en 0. Esto significa que el elemento 0 de nuestro tensor de \"salidas\" sigue siendo todo ceros. Así que nuestras `trg` y `outputs` se parecen a:\n",
    "\n",
    "$$\\begin{alinear*}\n",
    "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{resultados} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "Aquí, cuando calculamos la pérdida, cortamos el primer elemento de cada tensor para obtener:\n",
    "\n",
    "$$\\begin{alinear*}\n",
    "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{salidas} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "En cada iteración:\n",
    "- obtener las oraciones de origen y de destino del lote, $X$ y $Y$\n",
    "- poner a cero los gradientes calculados a partir del último lote\n",
    "- introduzca el origen y el destino en el modelo para obtener el resultado, $\\hat{Y}$\n",
    "- como la función de pérdida solo funciona en entradas 2d con objetivos 1d, necesitamos aplanar cada uno de ellos con `.view`\n",
    "     - cortamos la primera columna de los tensores de salida y destino como se mencionó anteriormente\n",
    "- calcula los gradientes con `loss.backward()`\n",
    "- recorte los gradientes para evitar que exploten (un problema común en RNN)\n",
    "- actualizar los parámetros de nuestro modelo haciendo un paso optimizador\n",
    "- sumar el valor de la pérdida a un total acumulado\n",
    "\n",
    "Finalmente, devolvemos la pérdida que se promedia en todos los batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a2c0aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:43.863678Z",
     "start_time": "2023-08-07T12:10:43.847678Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ae8296e47d370619304a765919b81b2",
     "grade": false,
     "grade_id": "cell-5e78bda9de1a9bb9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        # Aprox 1 linea para\n",
    "        # optimizer.zero...\n",
    "        # YOUR CODE HERE\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        # Aprox 1 linea para\n",
    "        # loss = \n",
    "        # YOUR CODE HERE\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc0050",
   "metadata": {},
   "source": [
    "Nuestro ciclo de evaluación es similar a nuestro ciclo de entrenamiento, sin embargo, como no estamos actualizando ningún parámetro, no necesitamos pasar un optimizador o un valor de clip.\n",
    "\n",
    "Debemos recordar poner el modelo en modo de evaluación con `model.eval()`. Esto desactivará el dropout (y la batch normalization, si se usa).\n",
    "\n",
    "Usamos el bloque `with torch.no_grad()` para garantizar que no se calculen gradientes dentro del bloque. Esto reduce el consumo de memoria y acelera el proceso.\n",
    "\n",
    "El ciclo de iteración es similar (sin las actualizaciones de parámetros); sin embargo, debemos asegurarnos de desactivar el forzado del maestro para la evaluación. }\n",
    "Esto hará que el modelo solo use sus propias predicciones para hacer más predicciones dentro de una oración, lo que refleja cómo se usaría en la implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75e62dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:44.241992Z",
     "start_time": "2023-08-07T12:10:44.225998Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e696c7",
   "metadata": {},
   "source": [
    "A continuación, crearemos una función que usaremos para decirnos cuánto tarda una época.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68230473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:10:44.738487Z",
     "start_time": "2023-08-07T12:10:44.726276Z"
    }
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe81d58",
   "metadata": {},
   "source": [
    "Ahora sí, ¡empecemos a entrenar a nuestro modelo!\n",
    "\n",
    "En cada época, comprobaremos si nuestro modelo ha logrado la mejor pérdida de validación hasta el momento. Si es así, actualizaremos nuestra mejor pérdida de validación y guardaremos los parámetros de nuestro modelo \n",
    "(llamado `state_dict` en PyTorch). Luego, cuando lleguemos a probar nuestro modelo, usaremos los parámetros guardados para lograr la mejor pérdida de validación.\n",
    "\n",
    "Estaremos mostrando tanto la pérdida como la perplejidad en cada época. Es más fácil ver un cambio en la perplejidad que un cambio en la pérdida ya que los números son mucho mayores.\n",
    "\n",
    "Ademas, cargaremos los parámetros (`state_dict`) que dieron a nuestro modelo la mejor pérdida de validación y ejecutaremos el modelo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b0215ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:18:56.768139Z",
     "start_time": "2023-08-07T12:10:46.646109Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3abe75f3dec72dbef634b76eecb3cb54",
     "grade": false,
     "grade_id": "cell-c2a7405dde118a6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 42s\n",
      "\tTrain Loss: 3.947 | Train PPL:  51.780\n",
      "\t Val. Loss: 4.433 |  Val. PPL:  84.225\n",
      "Epoch: 02 | Time: 0m 44s\n",
      "\tTrain Loss: 3.748 | Train PPL:  42.422\n",
      "\t Val. Loss: 4.301 |  Val. PPL:  73.800\n",
      "Epoch: 03 | Time: 0m 47s\n",
      "\tTrain Loss: 3.584 | Train PPL:  36.033\n",
      "\t Val. Loss: 4.155 |  Val. PPL:  63.743\n"
     ]
    }
   ],
   "source": [
    "# para que pueda definir\n",
    "N_EPOCHS = 3\n",
    "CLIP = 1\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71971818",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T22:26:39.976008Z",
     "start_time": "2023-08-07T22:26:39.653650Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f8a150031eecd843b79d77d31d64804",
     "grade": true,
     "grade_id": "cell-b70f37945f5a9981",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"25\"}--> \n",
       "         ✓ [25 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"alert alert-box alert-success\">\n",
       "        <h1> <!--{id:\"CORRECTMARK\", marks:\"25\"}--> \n",
       "         ✓ [25 marks] \n",
       "         </h1> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se valuara que el loss de training sea menor a 4 y el de validacion a 4.5\n",
    "\n",
    "with tick.marks(25):        \n",
    "    assert compare_numbers(new_representation(train_loss), \"3c3d\", '0x1.0000000000000p+2')\n",
    "    \n",
    "with tick.marks(25):        \n",
    "    assert compare_numbers(new_representation(valid_loss), \"3c3d\", '0x1.2000000000000p+2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c01e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:25:14.650820Z",
     "start_time": "2023-08-07T12:25:12.622015Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a77cbb7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58fd9a560ef4d1a143e87ce331286237",
     "grade": true,
     "grade_id": "cell-e94ae9af3a4c26ff",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tick' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLa fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m tick\u001b[39m.\u001b[39msummarise_marks() \u001b[39m# \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tick' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print()\n",
    "print(\"La fraccion de abajo muestra su rendimiento basado en las partes visibles de este laboratorio\")\n",
    "tick.summarise_marks() # "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
